{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DK_zgOn0ryT"
   },
   "source": [
    "# **Lab 2 Basim Sherief 1210207**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czaY5SFEp3Ec"
   },
   "source": [
    "### **Requirement 2**\n",
    "\n",
    "1) Complete the provided matrix addition example, following these cases:\n",
    "        \n",
    "        A.   kernel1: each thread produces one output matrix element\n",
    "        B.   kernel2: each thread produces one output matrix row\n",
    "        C.   kernel3: each thread produces one output matrix column\n",
    "  Analyze the pros and cons of each of the kernels above by using nvprof with large matrix sizes to validate your posize_ts. Collect your insights in a PDF report and explain them.\n",
    "\n",
    "2) Implement a matrixâ€“vector multiplication kernel. Use one thread to calculate an output vector element.\n",
    "\n",
    "Let both programs read testcases from a .txt file and prsize_t the output to another. Their pathes are to be provided as command line arguments. Sample test file and invoking command are to be attached to the e-learning page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:00.475818Z",
     "iopub.status.busy": "2025-02-17T11:09:00.475444Z",
     "iopub.status.idle": "2025-02-17T11:09:08.827427Z",
     "shell.execute_reply": "2025-02-17T11:09:08.826314Z",
     "shell.execute_reply.started": "2025-02-17T11:09:00.475768Z"
    },
    "id": "r9GPGdFOh_ma",
    "outputId": "2d2d04a0-e58a-4c27-e122-3086b281f62b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
      "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to c:\\users\\basim\\appdata\\local\\temp\\pip-req-build-i4rsh6yg\n",
      "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Source files will be saved in \"C:\\Users\\basim\\AppData\\Local\\Temp\\tmp42z12e90\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git 'C:\\Users\\basim\\AppData\\Local\\Temp\\pip-req-build-i4rsh6yg'\n"
     ]
    }
   ],
   "source": [
    "# Setup cuda environment\n",
    "%pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Testcases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:08.829464Z",
     "iopub.status.busy": "2025-02-17T11:09:08.829104Z",
     "iopub.status.idle": "2025-02-17T11:09:08.837875Z",
     "shell.execute_reply": "2025-02-17T11:09:08.836745Z",
     "shell.execute_reply.started": "2025-02-17T11:09:08.829423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_matrix_test_cases(num_tests, min_dim, max_dim, min_val, max_val, output_file):\n",
    "    \"\"\"\n",
    "    Generate test cases for matrix addition\n",
    "    Parameters:\n",
    "    - num_tests: number of test cases\n",
    "    - min_dim: minimum dimension (rows/cols)\n",
    "    - max_dim: maximum dimension (rows/cols)\n",
    "    - min_val: minimum value in matrices\n",
    "    - max_val: maximum value in matrices\n",
    "    - output_file: path to output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write number of test cases\n",
    "        f.write(f\"{num_tests}\\n\")\n",
    "        \n",
    "        for _ in range(num_tests):\n",
    "            # Generate random dimensions\n",
    "            rows = np.random.randint(min_dim, max_dim + 1)\n",
    "            cols = np.random.randint(min_dim, max_dim + 1)\n",
    "            \n",
    "            # Write dimensions\n",
    "            f.write(f\"{rows} {cols}\\n\")\n",
    "            \n",
    "            # Generate and write first matrix\n",
    "            matrix1 = np.random.uniform(min_val, max_val, (rows, cols))\n",
    "            for row in matrix1:\n",
    "                f.write(\" \".join(f\"{x:.3f}\" for x in row) + \"\\n\")\n",
    "            \n",
    "            # Generate and write second matrix\n",
    "            matrix2 = np.random.uniform(min_val, max_val, (rows, cols))\n",
    "            for row in matrix2:\n",
    "                f.write(\" \".join(f\"{x:.3f}\" for x in row) + \"\\n\")\n",
    "\n",
    "# Set your parameters here\n",
    "params = {\n",
    "    'num_tests': 5,              # Number of test cases\n",
    "    'min_dim': 2,               # Minimum matrix dimension\n",
    "    'max_dim': 10,               # Maximum matrix dimension\n",
    "    'min_val': -50000000.0,           # Minimum value in matrices\n",
    "    'max_val': 5000000000.0,            # Maximum value in matrices\n",
    "    'output_file': './inputfile.txt'  # Output file name\n",
    "}\n",
    "\n",
    "# Run the generator with the specified parameters\n",
    "if __name__ == \"__main__\":\n",
    "    generate_matrix_test_cases(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CPU Only**\n",
    "# Vector addition in pure C (CPU-only execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:08.840091Z",
     "iopub.status.busy": "2025-02-17T11:09:08.839872Z",
     "iopub.status.idle": "2025-02-17T11:09:08.861102Z",
     "shell.execute_reply": "2025-02-17T11:09:08.860246Z",
     "shell.execute_reply.started": "2025-02-17T11:09:08.840072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernel0.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernel0.cu   \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <assert.h>\n",
    "#include <time.h>\n",
    "\n",
    "#define MAX_ERR 1e-6\n",
    "\n",
    "// Function to perform vector addition\n",
    "void vector_add(double *out, double *a, double *b, size_t  n) {\n",
    "    for (size_t  i = 0; i < n; i++) {\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "    FILE *file_reading;\n",
    "    int numberOfTests;\n",
    "    size_t rows, cols;\n",
    "    // Open the file in read mode\n",
    "    file_reading = fopen(argv[1], \"r\");\n",
    "    if (file_reading == NULL) {\n",
    "        printf(\"Error opening file!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    // Read number of tests\n",
    "    fscanf(file_reading, \"%d\",&numberOfTests);\n",
    "for(size_t i=0;i<numberOfTests;i++){\n",
    "    \n",
    "    // Read matrix dimensions\n",
    "    fscanf(file_reading, \"%zu %zu\",&rows, &cols);\n",
    "    // Allocate host matrices\n",
    "    double* A = (double*)malloc(sizeof(double) * rows * cols);\n",
    "    double* B = (double*)malloc(sizeof(double) * rows * cols);\n",
    "    double* C = (double*)malloc(sizeof(double) * rows * cols);\n",
    "\n",
    "    if (A == NULL || B == NULL || C == NULL) {\n",
    "        printf(\"Memory allocation failed!\\n\");\n",
    "        fclose(file_reading);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    // Read matrices A and B\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &A[i]);\n",
    "    }\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &B[i]);\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    // Start timing\n",
    "    clock_t start = clock();\n",
    "\n",
    "    // Perform vector addition\n",
    "    vector_add(C, A, B, rows * cols);\n",
    "\n",
    "    // End timing\n",
    "    clock_t end = clock();\n",
    "\n",
    "    // Calculate the elapsed time in seconds\n",
    "    double time_spent = (double)(end - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "\n",
    "    printf(\"Time elapsed: %f ms\\n\", time_spent);\n",
    "\n",
    "    // Verification\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        assert(fabs(C[i] - A[i] - B[i]) < MAX_ERR);\n",
    "    }\n",
    "\n",
    "    printf(\"Vector addition completed successfully!\\n\");\n",
    "\n",
    "  \n",
    "    // Write results to output file\n",
    "   // Write results to output file\n",
    "    FILE *file_writing;\n",
    "    file_writing= fopen(argv[2], \"w\"); // Open file for writing\n",
    "    if (file_writing == NULL) {\n",
    "        perror(\"Error opening file\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "\n",
    "    // Write matrix C\n",
    "    for (size_t i = 0; i < rows; i++) {\n",
    "        for (size_t j = 0; j < cols; j++) {\n",
    "              printf(\"%.3lf \", C[i * cols + j]);    \n",
    "            fprintf(file_writing, \"%.3lf \", C[i * cols + j]); // Write double with 2 decimal places\n",
    "        }\n",
    "         printf(\"\\n\");  \n",
    "        fprintf(file_writing, \"\\n\"); // New line after each row\n",
    "    }\n",
    "    fclose(file_writing);\n",
    "\n",
    "\n",
    "    // Free allocated memory\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "}\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:08.862674Z",
     "iopub.status.busy": "2025-02-17T11:09:08.862362Z",
     "iopub.status.idle": "2025-02-17T11:09:10.391461Z",
     "shell.execute_reply": "2025-02-17T11:09:10.390549Z",
     "shell.execute_reply.started": "2025-02-17T11:09:08.862649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel0.cu\n",
      "tmpxft_00001b88_00000000-10_kernel0.cudafe1.cpp\n",
      "   Creating library kernel0.lib and object kernel0.exp\n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "6435366487.397 1433178484.549 1510275566.968 1828845876.877 4483950868.614 4166451532.966 7074156538.042 4541242014.452 4837375348.743 8117219560.029 \n",
      "6575081993.646 4886121097.452 4007272978.500 4812473309.278 5173170920.441 846818149.884 9062404273.921 6520157039.148 2726124482.217 5840637726.005 \n",
      "8009372404.035 4441444876.484 5269302642.615 4512346469.820 2819923498.570 4296742355.366 2681748014.299 5074690285.724 6157783623.263 2653662371.780 \n",
      "4151113376.366 9019137420.064 2907495850.631 5218992329.786 6116726160.825 3995332147.349 6218736239.915 3158080352.853 6635297222.719 4046196482.220 \n",
      "3829458124.007 771164488.591 5023396724.300 3939093889.824 3665183783.752 4810321684.818 4053150843.068 5071842735.070 5408255950.289 5519999999.163 \n",
      "5361094170.047 8661846465.107 1760658262.295 4708091841.541 4797400952.902 7130418557.950 5559034200.029 6038905640.509 6143575485.546 2105827621.119 \n",
      "2711240274.821 3438371894.433 4482970018.522 7037642620.151 6511639637.238 6548131124.076 6965477208.452 2486904737.663 4939482629.912 3070156531.701 \n",
      "3714016329.152 2494660031.703 6802210819.632 6881349371.759 5441786007.441 8436251549.750 4623644671.841 1794878768.597 2227071535.590 3217372215.350 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "761213067.824 2511538058.749 5984828332.341 9391841632.707 4204815676.807 6580517731.218 9369776627.030 \n",
      "3637485491.469 5052790127.499 7666186334.888 7902907684.605 1123605453.713 1701990505.936 5545637186.170 \n",
      "6214019548.948 3302138145.943 8614216722.988 4510045924.739 8164505797.788 2288789871.459 3151119934.758 \n",
      "5902274048.062 4618173451.001 1576859150.133 7971786212.831 2675723162.141 4311039097.364 5288495713.933 \n",
      "6078282201.790 4235231782.267 4993234873.776 1440955076.810 5554533638.240 4073563809.191 8189496626.855 \n",
      "5095072681.392 5198931962.607 5956039528.225 7234395352.883 6445426358.161 7352648184.355 7184603140.136 \n",
      "1135122392.011 5966504241.332 4440505011.523 4814178227.628 1956477440.713 4931024316.870 6209808566.133 \n",
      "6764526262.681 4045465559.403 6011357223.802 5192454178.581 5122733288.271 4132624217.360 1372463060.505 \n",
      "2691679316.677 6894050764.471 8082532046.059 3867567830.401 5141832478.331 4911752892.336 5284381173.341 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "3300374591.918 8313587150.963 3722770678.623 5353229735.435 5285507391.865 3581271555.045 8158049158.077 1490736635.474 1316926458.317 \n",
      "1694169144.344 6093724028.741 3772054210.952 7321005090.373 1486438192.207 4152076058.493 4875543284.548 2549411436.797 4103287976.667 \n",
      "4438397122.925 3421946500.582 7292164668.727 4773068523.679 5798608751.632 1398417967.948 6489750365.624 5111061100.557 7636672853.566 \n",
      "6196028395.692 5857685246.327 1258439337.383 3667128297.752 6618239007.060 3257692853.226 8819547543.657 4711882393.938 5192196005.082 \n",
      "6937209446.526 5218511429.952 5060981569.750 4913617396.337 7585326554.484 4880954005.982 3460629499.315 5900342531.609 3814927410.853 \n",
      "6425364187.996 2725958431.788 7232040143.604 9179238488.715 8592621886.467 2868984882.165 2831060042.305 5239745088.839 4026271544.181 \n",
      "2721439013.686 7628971985.869 7892712647.132 5290435449.392 6367458230.121 4039660718.808 8718814106.585 7422855296.206 583953983.006 \n",
      "5968644727.316 2507882636.319 3844788097.152 5064848607.021 4767280796.528 4656258333.251 3810125213.260 4296546420.413 3910788285.776 \n",
      "8735842709.694 1929388076.696 5707585232.183 8465863968.104 2894823680.873 3329859147.697 2780580520.879 1536565273.210 5836021703.002 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "213233863.526 3367811272.706 1376657602.818 \n",
      "6018966047.050 9173534922.594 8481540812.677 \n",
      "5335558716.138 1801948790.553 6398284847.429 \n",
      "4463688786.522 2853217005.765 7583230807.514 \n",
      "5781869321.479 3515802152.626 8165245624.137 \n",
      "1498537462.183 3075417108.919 4940576498.207 \n",
      "5169761300.721 6351200572.239 7299581078.417 \n",
      "6708656126.687 8677950999.304 4075768469.990 \n",
      "4221191865.154 3224299925.325 2460556115.643 \n",
      "5828683597.164 4431646967.095 7463764528.801 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "7949955341.998 5155542545.144 5358607835.766 6305562056.000 835230146.178 5499518424.353 \n",
      "6506006639.108 1500406476.312 4537344650.210 7894487118.232 1565467801.948 1708371606.834 \n",
      "1848141973.997 5866024405.197 8012521851.745 8571619761.109 2563769862.090 1542424956.133 \n"
     ]
    }
   ],
   "source": [
    "# Compile the CUDA program\n",
    "!nvcc kernel0.cu -o kernel0.exe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "6435366487.397 1433178484.549 1510275566.968 1828845876.877 4483950868.614 4166451532.966 7074156538.042 4541242014.452 4837375348.743 8117219560.029 \n",
      "6575081993.646 4886121097.452 4007272978.500 4812473309.278 5173170920.441 846818149.884 9062404273.921 6520157039.148 2726124482.217 5840637726.005 \n",
      "8009372404.035 4441444876.484 5269302642.615 4512346469.820 2819923498.570 4296742355.366 2681748014.299 5074690285.724 6157783623.263 2653662371.780 \n",
      "4151113376.366 9019137420.064 2907495850.631 5218992329.786 6116726160.825 3995332147.349 6218736239.915 3158080352.853 6635297222.719 4046196482.220 \n",
      "3829458124.007 771164488.591 5023396724.300 3939093889.824 3665183783.752 4810321684.818 4053150843.068 5071842735.070 5408255950.289 5519999999.163 \n",
      "5361094170.047 8661846465.107 1760658262.295 4708091841.541 4797400952.902 7130418557.950 5559034200.029 6038905640.509 6143575485.546 2105827621.119 \n",
      "2711240274.821 3438371894.433 4482970018.522 7037642620.151 6511639637.238 6548131124.076 6965477208.452 2486904737.663 4939482629.912 3070156531.701 \n",
      "3714016329.152 2494660031.703 6802210819.632 6881349371.759 5441786007.441 8436251549.750 4623644671.841 1794878768.597 2227071535.590 3217372215.350 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "761213067.824 2511538058.749 5984828332.341 9391841632.707 4204815676.807 6580517731.218 9369776627.030 \n",
      "3637485491.469 5052790127.499 7666186334.888 7902907684.605 1123605453.713 1701990505.936 5545637186.170 \n",
      "6214019548.948 3302138145.943 8614216722.988 4510045924.739 8164505797.788 2288789871.459 3151119934.758 \n",
      "5902274048.062 4618173451.001 1576859150.133 7971786212.831 2675723162.141 4311039097.364 5288495713.933 \n",
      "6078282201.790 4235231782.267 4993234873.776 1440955076.810 5554533638.240 4073563809.191 8189496626.855 \n",
      "5095072681.392 5198931962.607 5956039528.225 7234395352.883 6445426358.161 7352648184.355 7184603140.136 \n",
      "1135122392.011 5966504241.332 4440505011.523 4814178227.628 1956477440.713 4931024316.870 6209808566.133 \n",
      "6764526262.681 4045465559.403 6011357223.802 5192454178.581 5122733288.271 4132624217.360 1372463060.505 \n",
      "2691679316.677 6894050764.471 8082532046.059 3867567830.401 5141832478.331 4911752892.336 5284381173.341 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "3300374591.918 8313587150.963 3722770678.623 5353229735.435 5285507391.865 3581271555.045 8158049158.077 1490736635.474 1316926458.317 \n",
      "1694169144.344 6093724028.741 3772054210.952 7321005090.373 1486438192.207 4152076058.493 4875543284.548 2549411436.797 4103287976.667 \n",
      "4438397122.925 3421946500.582 7292164668.727 4773068523.679 5798608751.632 1398417967.948 6489750365.624 5111061100.557 7636672853.566 \n",
      "6196028395.692 5857685246.327 1258439337.383 3667128297.752 6618239007.060 3257692853.226 8819547543.657 4711882393.938 5192196005.082 \n",
      "6937209446.526 5218511429.952 5060981569.750 4913617396.337 7585326554.484 4880954005.982 3460629499.315 5900342531.609 3814927410.853 \n",
      "6425364187.996 2725958431.788 7232040143.604 9179238488.715 8592621886.467 2868984882.165 2831060042.305 5239745088.839 4026271544.181 \n",
      "2721439013.686 7628971985.869 7892712647.132 5290435449.392 6367458230.121 4039660718.808 8718814106.585 7422855296.206 583953983.006 \n",
      "5968644727.316 2507882636.319 3844788097.152 5064848607.021 4767280796.528 4656258333.251 3810125213.260 4296546420.413 3910788285.776 \n",
      "8735842709.694 1929388076.696 5707585232.183 8465863968.104 2894823680.873 3329859147.697 2780580520.879 1536565273.210 5836021703.002 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "213233863.526 3367811272.706 1376657602.818 \n",
      "6018966047.050 9173534922.594 8481540812.677 \n",
      "5335558716.138 1801948790.553 6398284847.429 \n",
      "4463688786.522 2853217005.765 7583230807.514 \n",
      "5781869321.479 3515802152.626 8165245624.137 \n",
      "1498537462.183 3075417108.919 4940576498.207 \n",
      "5169761300.721 6351200572.239 7299581078.417 \n",
      "6708656126.687 8677950999.304 4075768469.990 \n",
      "4221191865.154 3224299925.325 2460556115.643 \n",
      "5828683597.164 4431646967.095 7463764528.801 \n",
      "Time elapsed: 0.000000 ms\n",
      "Vector addition completed successfully!\n",
      "7949955341.998 5155542545.144 5358607835.766 6305562056.000 835230146.178 5499518424.353 \n",
      "6506006639.108 1500406476.312 4537344650.210 7894487118.232 1565467801.948 1708371606.834 \n",
      "1848141973.997 5866024405.197 8012521851.745 8571619761.109 2563769862.090 1542424956.133 \n"
     ]
    }
   ],
   "source": [
    "# Run the executable (Windows style)\n",
    "!.\\kernel0.exe inputfile.txt outputfile_cpu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For profiling with nvprof (Windows style)\n",
    "!nvprof .\\kernel0.exe inputfile.txt outputfile_cpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel1: each thread produces one output matrix element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:10.393024Z",
     "iopub.status.busy": "2025-02-17T11:09:10.392688Z",
     "iopub.status.idle": "2025-02-17T11:09:10.399670Z",
     "shell.execute_reply": "2025-02-17T11:09:10.398888Z",
     "shell.execute_reply.started": "2025-02-17T11:09:10.392993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernel1.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernel1.cu   \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <assert.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <fstream>\n",
    "#include <iostream>\n",
    "#include <sstream>\n",
    "#define MAX_ERR 1e-6\n",
    "__global__ void matrixAddKernel1(double* C, double* A, double* B, size_t rows, size_t cols) {\n",
    "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (row < rows && col < cols) {\n",
    "        size_t idx = row * cols + col;\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "cudaError_t addMatricesWithCuda(double* C, double* A, double* B, size_t rows, size_t cols) {\n",
    "    double* dev_A = nullptr;\n",
    "    double* dev_B = nullptr;\n",
    "    double* dev_C = nullptr;\n",
    "    cudaError_t cudaStatus;\n",
    "\n",
    "    // Allocate GPU buffers\n",
    "    size_t size = rows * cols * sizeof(double);  // Changed from size_t to double\n",
    "    \n",
    "    cudaStatus = cudaMalloc((void**)&dev_C, size);\n",
    "    \n",
    "    cudaStatus = cudaMalloc((void**)&dev_A, size);\n",
    "\n",
    "    cudaStatus = cudaMalloc((void**)&dev_B, size);\n",
    "\n",
    "    // Copy input matrices from host memory to GPU buffers\n",
    "    cudaStatus = cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);    \n",
    "    cudaStatus = cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    dim3 threadsPerBlock(16, 16); // i just did what Ta and cuda said the best to use\n",
    "    dim3 numBlocks((cols + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                   (rows + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrixAddKernel1<<<numBlocks, threadsPerBlock>>>(dev_C, dev_A, dev_B, rows, cols);\n",
    "\n",
    "    // Copy output matrix from GPU buffer to host memory\n",
    "    cudaStatus = cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    return cudaStatus;\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "\n",
    "    FILE *file_reading;\n",
    "    int numberOfTests;\n",
    "    size_t  rows, cols;\n",
    "    // Open the file in read mode\n",
    "    file_reading = fopen(argv[1], \"r\");\n",
    "    if (file_reading == NULL) {\n",
    "        printf(\"Error opening file!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    // Read number of tests\n",
    "    fscanf(file_reading, \"%d\",&numberOfTests);\n",
    "for(size_t i=0;i<numberOfTests;i++){\n",
    "    \n",
    "    // Read matrix dimensions\n",
    "    fscanf(file_reading, \"%zu %zu\", &rows, &cols);\n",
    "\n",
    "    // Allocate host matrices\n",
    "    double* A = (double*)malloc(sizeof(double) * rows * cols);  // Changed from size_t to double\n",
    "    double* B = (double*)malloc(sizeof(double) * rows * cols);\n",
    "    double* C = (double*)malloc(sizeof(double) * rows * cols);\n",
    "\n",
    "    // Read matrices A and B\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &A[i]);\n",
    "    }\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &B[i]);\n",
    "    }\n",
    "    \n",
    " \n",
    "\n",
    "    // Add matrices using CUDA\n",
    "    cudaError_t cudaStatus = addMatricesWithCuda(C, A, B, rows, cols);\n",
    "\n",
    "    // Verification\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        assert(fabs(C[i] - A[i] - B[i]) < MAX_ERR);\n",
    "    }\n",
    "\n",
    "    printf(\"Vector addition completed successfully!\\n\");\n",
    "\n",
    "    // Write results to output file\n",
    "    FILE *file_writing;\n",
    "    file_writing= fopen(argv[2], \"w\"); // Open file for writing\n",
    "    if (file_writing == NULL) {\n",
    "        perror(\"Error opening file\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "\n",
    "    // Write matrix C\n",
    "    for (size_t i = 0; i < rows; i++) {\n",
    "        for (size_t j = 0; j < cols; j++) {\n",
    "              printf(\"%.3f \", C[i * cols + j]);    \n",
    "            fprintf(file_writing, \"%.3f \", C[i * cols + j]); // Write double with 2 decimal places\n",
    "        }\n",
    "         printf(\"\\n\");  \n",
    "        fprintf(file_writing, \"\\n\"); // New line after each row\n",
    "    }\n",
    "    fclose(file_writing);\n",
    "\n",
    "\n",
    "    // Cleanup\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "}\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:10.400617Z",
     "iopub.status.busy": "2025-02-17T11:09:10.400423Z",
     "iopub.status.idle": "2025-02-17T11:09:12.966248Z",
     "shell.execute_reply": "2025-02-17T11:09:12.965410Z",
     "shell.execute_reply.started": "2025-02-17T11:09:10.400599Z"
    },
    "id": "vmzESAnuV15c",
    "outputId": "5725348d-ad4c-4c8d-c43c-f993c9ce997d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the CUDA program\n",
    "!nvcc kernel1.cu -o kernel1.exe\n",
    "\n",
    "# Run the executable (Windows style)\n",
    "!.\\kernel1.exe inputfile.txt outputfile_cpu.txt\n",
    "\n",
    "# perfomance for cuda is depricacted but can be used in kaggle notebook\n",
    "# !nvprof .\\kernel1.exe inputfile.txt outputfile_cpu.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 12764 (E:\\02_Learn\\01_University\\Senior-1 Spring\\Current\\Parallel Computing\\Labs\\Lab_2_Notebook_Solution\\kernel1.exe)\n",
      "==PROF== Profiling \"matrixAddKernel1\" - 0: 0%....50%....100% - 39 passes\n",
      "==PROF== Profiling \"matrixAddKernel1\" - 1: 0%....50%....100% - 39 passes\n",
      "==PROF== Profiling \"matrixAddKernel1\" - 2: 0%....50%....100% - 39 passes\n",
      "==PROF== Profiling \"matrixAddKernel1\" - 3: 0%....50%....100% - 39 passes\n",
      "==PROF== Profiling \"matrixAddKernel1\" - 4: 0%....50%....100% - 39 passes\n",
      "Vector addition completed successfully!\n",
      "6435366487.397 1433178484.549 1510275566.968 1828845876.877 4483950868.614 4166451532.966 7074156538.042 4541242014.452 4837375348.743 8117219560.029 \n",
      "6575081993.646 4886121097.452 4007272978.500 4812473309.278 5173170920.441 846818149.884 9062404273.921 6520157039.148 2726124482.217 5840637726.005 \n",
      "8009372404.035 4441444876.484 5269302642.615 4512346469.820 2819923498.570 4296742355.366 2681748014.299 5074690285.724 6157783623.263 2653662371.780 \n",
      "4151113376.366 9019137420.064 2907495850.631 5218992329.786 6116726160.825 3995332147.349 6218736239.915 3158080352.853 6635297222.719 4046196482.220 \n",
      "3829458124.007 771164488.591 5023396724.300 3939093889.824 3665183783.752 4810321684.818 4053150843.068 5071842735.070 5408255950.289 5519999999.163 \n",
      "5361094170.047 8661846465.107 1760658262.295 4708091841.541 4797400952.902 7130418557.950 5559034200.029 6038905640.509 6143575485.546 2105827621.119 \n",
      "2711240274.821 3438371894.433 4482970018.522 7037642620.151 6511639637.238 6548131124.076 6965477208.452 2486904737.663 4939482629.912 3070156531.701 \n",
      "3714016329.152 2494660031.703 6802210819.632 6881349371.759 5441786007.441 8436251549.750 4623644671.841 1794878768.597 2227071535.590 3217372215.350 \n",
      "Vector addition completed successfully!\n",
      "761213067.824 2511538058.749 5984828332.341 9391841632.707 4204815676.807 6580517731.218 9369776627.030 \n",
      "3637485491.469 5052790127.499 7666186334.888 7902907684.605 1123605453.713 1701990505.936 5545637186.170 \n",
      "6214019548.948 3302138145.943 8614216722.988 4510045924.739 8164505797.788 2288789871.459 3151119934.758 \n",
      "5902274048.062 4618173451.001 1576859150.133 7971786212.831 2675723162.141 4311039097.364 5288495713.933 \n",
      "6078282201.790 4235231782.267 4993234873.776 1440955076.810 5554533638.240 4073563809.191 8189496626.855 \n",
      "5095072681.392 5198931962.607 5956039528.225 7234395352.883 6445426358.161 7352648184.355 7184603140.136 \n",
      "1135122392.011 5966504241.332 4440505011.523 4814178227.628 1956477440.713 4931024316.870 6209808566.133 \n",
      "6764526262.681 4045465559.403 6011357223.802 5192454178.581 5122733288.271 4132624217.360 1372463060.505 \n",
      "2691679316.677 6894050764.471 8082532046.059 3867567830.401 5141832478.331 4911752892.336 5284381173.341 \n",
      "Vector addition completed successfully!\n",
      "3300374591.918 8313587150.963 3722770678.623 5353229735.435 5285507391.865 3581271555.045 8158049158.077 1490736635.474 1316926458.317 \n",
      "1694169144.344 6093724028.741 3772054210.952 7321005090.373 1486438192.207 4152076058.493 4875543284.548 2549411436.797 4103287976.667 \n",
      "4438397122.925 3421946500.582 7292164668.727 4773068523.679 5798608751.632 1398417967.948 6489750365.624 5111061100.557 7636672853.566 \n",
      "6196028395.692 5857685246.327 1258439337.383 3667128297.752 6618239007.060 3257692853.226 8819547543.657 4711882393.938 5192196005.082 \n",
      "6937209446.526 5218511429.952 5060981569.750 4913617396.337 7585326554.484 4880954005.982 3460629499.315 5900342531.609 3814927410.853 \n",
      "6425364187.996 2725958431.788 7232040143.604 9179238488.715 8592621886.467 2868984882.165 2831060042.305 5239745088.839 4026271544.181 \n",
      "2721439013.686 7628971985.869 7892712647.132 5290435449.392 6367458230.121 4039660718.808 8718814106.585 7422855296.206 583953983.006 \n",
      "5968644727.316 2507882636.319 3844788097.152 5064848607.021 4767280796.528 4656258333.251 3810125213.260 4296546420.413 3910788285.776 \n",
      "8735842709.694 1929388076.696 5707585232.183 8465863968.104 2894823680.873 3329859147.697 2780580520.879 1536565273.210 5836021703.002 \n",
      "Vector addition completed successfully!\n",
      "213233863.526 3367811272.706 1376657602.818 \n",
      "6018966047.050 9173534922.594 8481540812.677 \n",
      "5335558716.138 1801948790.553 6398284847.429 \n",
      "4463688786.522 2853217005.765 7583230807.514 \n",
      "5781869321.479 3515802152.626 8165245624.137 \n",
      "1498537462.183 3075417108.919 4940576498.207 \n",
      "5169761300.721 6351200572.239 7299581078.417 \n",
      "6708656126.687 8677950999.304 4075768469.990 \n",
      "4221191865.154 3224299925.325 2460556115.643 \n",
      "5828683597.164 4431646967.095 7463764528.801 \n",
      "Vector addition completed successfully!\n",
      "7949955341.998 5155542545.144 5358607835.766 6305562056.000 835230146.178 5499518424.353 \n",
      "6506006639.108 1500406476.312 4537344650.210 7894487118.232 1565467801.948 1708371606.834 \n",
      "1848141973.997 5866024405.197 8012521851.745 8571619761.109 2563769862.090 1542424956.133 \n",
      "==PROF== Disconnected from process 12764\n",
      "[12764] kernel1.exe@127.0.0.1\n",
      "  matrixAddKernel1(double *, double *, double *, unsigned long long, unsigned long long) (1, 1, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         7.77\n",
      "    SM Frequency                    Ghz         1.56\n",
      "    Elapsed Cycles                cycle        3,554\n",
      "    Memory Throughput                 %         0.82\n",
      "    DRAM Throughput                   %         0.82\n",
      "    Duration                         us         2.27\n",
      "    L1/TEX Cache Throughput           %        17.05\n",
      "    L2 Cache Throughput               %         0.48\n",
      "    SM Active Cycles              cycle        70.38\n",
      "    Compute (SM) Throughput           %         0.08\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of \n",
      "          this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling   \n",
      "          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         \n",
      "          roofline analysis.                                                                                            \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Mbyte         2.10\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval          us            1\n",
      "    # Pass Groups                                    2\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     \n",
      "          samples.                                                                                                      \n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.10\n",
      "    Executed Ipc Elapsed  inst/cycle         0.00\n",
      "    Issue Slots Busy               %         3.08\n",
      "    Issued Ipc Active     inst/cycle         0.12\n",
      "    SM Busy                        %         3.79\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.21%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    --------------------------- ----------- ------------\n",
      "    Metric Name                 Metric Unit Metric Value\n",
      "    --------------------------- ----------- ------------\n",
      "    Memory Throughput               Gbyte/s         2.03\n",
      "    Mem Busy                              %         0.48\n",
      "    Max Bandwidth                         %         0.82\n",
      "    L1/TEX Hit Rate                       %            0\n",
      "    L2 Compression Success Rate           %            0\n",
      "    L2 Compression Ratio                               0\n",
      "    L2 Hit Rate                           %        71.86\n",
      "    Mem Pipes Busy                        %         0.07\n",
      "    --------------------------- ----------- ------------\n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %         3.20\n",
      "    Issued Warp Per Scheduler                        0.03\n",
      "    No Eligible                            %        96.80\n",
      "    Active Warps Per Scheduler          warp         1.59\n",
      "    Eligible Warps Per Scheduler        warp         0.03\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.8%                                                                                     \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 31.2 cycles. This might leave hardware resources underutilized and may lead to    \n",
      "          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   \n",
      "          1.59 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        49.65\n",
      "    Warp Cycles Per Executed Instruction           cycle        62.98\n",
      "    Avg. Active Threads Per Warp                                27.02\n",
      "    Avg. Not Predicated Off Threads Per Warp                    26.54\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 57.73%                                                                                          \n",
      "          On average, each warp of this kernel spends 28.7 cycles being stalled waiting for an immediate constant cache \n",
      "          (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;        \n",
      "          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      \n",
      "          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    \n",
      "          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       \n",
      "          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   \n",
      "          threads of a warp access the same location, then constant memory can be as fast as a register access. This    \n",
      "          stall type represents about 57.7% of the total average of 49.7 cycles between issuing two instructions.       \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst         1.71\n",
      "    Executed Instructions                           inst          164\n",
      "    Avg. Issued Instructions Per Scheduler          inst         2.17\n",
      "    Issued Instructions                             inst          208\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 1.895%                                                                                          \n",
      "          This kernel executes 0 fused and 4 non-fused FP64 instructions. By converting pairs of non-fused instructions \n",
      "          to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput      \n",
      "          equivalent, the achieved FP64 performance could be increased by up to 50% (relative to its current            \n",
      "          performance). Check the Source page to identify where this kernel executes FP64 instructions.                 \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           16.38\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              24\n",
      "    Threads                                   thread             256\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.01\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 95.83%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           24\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            6\n",
      "    Theoretical Active Warps per SM        warp           48\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        12.63\n",
      "    Achieved Active Warps Per SM           warp         6.06\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 87.37%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.6%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle          144\n",
      "    Total DRAM Elapsed Cycles        cycle       70,656\n",
      "    Average L1 Active Cycles         cycle        70.38\n",
      "    Total L1 Elapsed Cycles          cycle       85,296\n",
      "    Average L2 Active Cycles         cycle       324.81\n",
      "    Total L2 Elapsed Cycles          cycle       55,376\n",
      "    Average SM Active Cycles         cycle        70.38\n",
      "    Total SM Elapsed Cycles          cycle       85,296\n",
      "    Average SMSP Active Cycles       cycle        67.70\n",
      "    Total SMSP Elapsed Cycles        cycle      341,184\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 5.379%                                                                                          \n",
      "          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     \n",
      "          Maximum instance value is 57.32% above the average, while the minimum instance value is 87.07% below the      \n",
      "          average.                                                                                                      \n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.07\n",
      "    Branch Instructions              inst           12\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "  matrixAddKernel1(double *, double *, double *, unsigned long long, unsigned long long) (1, 1, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         7.89\n",
      "    SM Frequency                    Ghz         1.58\n",
      "    Elapsed Cycles                cycle        3,589\n",
      "    Memory Throughput                 %         0.76\n",
      "    DRAM Throughput                   %         0.76\n",
      "    Duration                         us         2.27\n",
      "    L1/TEX Cache Throughput           %        16.90\n",
      "    L2 Cache Throughput               %         0.47\n",
      "    SM Active Cycles              cycle           71\n",
      "    Compute (SM) Throughput           %         0.09\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of \n",
      "          this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling   \n",
      "          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         \n",
      "          roofline analysis.                                                                                            \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Mbyte         2.10\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval          us            1\n",
      "    # Pass Groups                                    2\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     \n",
      "          samples.                                                                                                      \n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.11\n",
      "    Executed Ipc Elapsed  inst/cycle         0.00\n",
      "    Issue Slots Busy               %         3.33\n",
      "    Issued Ipc Active     inst/cycle         0.13\n",
      "    SM Busy                        %         4.69\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 95.31%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    --------------------------- ----------- ------------\n",
      "    Metric Name                 Metric Unit Metric Value\n",
      "    --------------------------- ----------- ------------\n",
      "    Memory Throughput               Gbyte/s         1.92\n",
      "    Mem Busy                              %         0.47\n",
      "    Max Bandwidth                         %         0.76\n",
      "    L1/TEX Hit Rate                       %         9.26\n",
      "    L2 Compression Success Rate           %            0\n",
      "    L2 Compression Ratio                               0\n",
      "    L2 Hit Rate                           %        72.61\n",
      "    Mem Pipes Busy                        %         0.07\n",
      "    --------------------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Tables\n",
      "    OPT   Est. Speedup: 0.05195%                                                                                        \n",
      "          The memory access pattern for global loads from L2 might not be optimal. On average, only 28.0 of the 32      \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 88.9% of sectors missed in      \n",
      "          L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for       \n",
      "          uncoalesced global loads.                                                                                     \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Speedup: 0.0896%                                                                                         \n",
      "          The memory access pattern for global stores to DRAM might not be optimal. On average, only 28.0 of the 32     \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 0.0% of sectors missed in L2.   \n",
      "          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  \n",
      "          global stores.                                                                                                \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %         3.48\n",
      "    Issued Warp Per Scheduler                        0.03\n",
      "    No Eligible                            %        96.52\n",
      "    Active Warps Per Scheduler          warp         1.66\n",
      "    Eligible Warps Per Scheduler        warp         0.04\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.52%                                                                                    \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 28.8 cycles. This might leave hardware resources underutilized and may lead to    \n",
      "          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   \n",
      "          1.66 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        47.68\n",
      "    Warp Cycles Per Executed Instruction           cycle        59.80\n",
      "    Avg. Active Threads Per Warp                                22.89\n",
      "    Avg. Not Predicated Off Threads Per Warp                    22.54\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 55.41%                                                                                          \n",
      "          On average, each warp of this kernel spends 26.4 cycles being stalled waiting for an immediate constant cache \n",
      "          (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;        \n",
      "          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      \n",
      "          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    \n",
      "          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       \n",
      "          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   \n",
      "          threads of a warp access the same location, then constant memory can be as fast as a register access. This    \n",
      "          stall type represents about 55.4% of the total average of 47.7 cycles between issuing two instructions.       \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Speedup: 0.02747%                                                                                        \n",
      "          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n",
      "          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n",
      "          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n",
      "          per cycle. This kernel achieves an average of 22.9 threads being active per cycle. This is further reduced    \n",
      "          to 22.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n",
      "          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n",
      "          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst         1.89\n",
      "    Executed Instructions                           inst          181\n",
      "    Avg. Issued Instructions Per Scheduler          inst         2.36\n",
      "    Issued Instructions                             inst          227\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 2.347%                                                                                          \n",
      "          This kernel executes 0 fused and 5 non-fused FP64 instructions. By converting pairs of non-fused instructions \n",
      "          to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput      \n",
      "          equivalent, the achieved FP64 performance could be increased by up to 50% (relative to its current            \n",
      "          performance). Check the Source page to identify where this kernel executes FP64 instructions.                 \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           16.38\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              24\n",
      "    Threads                                   thread             256\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.01\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 95.83%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           24\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            6\n",
      "    Theoretical Active Warps per SM        warp           48\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        13.60\n",
      "    Achieved Active Warps Per SM           warp         6.53\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 86.4%                                                                                           \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (13.6%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle          136\n",
      "    Total DRAM Elapsed Cycles        cycle       71,680\n",
      "    Average L1 Active Cycles         cycle           71\n",
      "    Total L1 Elapsed Cycles          cycle       86,088\n",
      "    Average L2 Active Cycles         cycle       336.44\n",
      "    Total L2 Elapsed Cycles          cycle       55,824\n",
      "    Average SM Active Cycles         cycle           71\n",
      "    Total SM Elapsed Cycles          cycle       86,088\n",
      "    Average SMSP Active Cycles       cycle        67.99\n",
      "    Total SMSP Elapsed Cycles        cycle      344,352\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 6.408%                                                                                          \n",
      "          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     \n",
      "          Maximum instance value is 66.46% above the average, while the minimum instance value is 87.52% below the      \n",
      "          average.                                                                                                      \n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.07\n",
      "    Branch Instructions              inst           13\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             \n",
      "\n",
      "  matrixAddKernel1(double *, double *, double *, unsigned long long, unsigned long long) (1, 1, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         7.89\n",
      "    SM Frequency                    Ghz         1.58\n",
      "    Elapsed Cycles                cycle        3,583\n",
      "    Memory Throughput                 %         0.83\n",
      "    DRAM Throughput                   %         0.83\n",
      "    Duration                         us         2.27\n",
      "    L1/TEX Cache Throughput           %        16.87\n",
      "    L2 Cache Throughput               %         0.49\n",
      "    SM Active Cycles              cycle        71.12\n",
      "    Compute (SM) Throughput           %         0.09\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of \n",
      "          this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling   \n",
      "          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         \n",
      "          roofline analysis.                                                                                            \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Mbyte         2.10\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval          us            1\n",
      "    # Pass Groups                                    2\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     \n",
      "          samples.                                                                                                      \n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.11\n",
      "    Executed Ipc Elapsed  inst/cycle         0.00\n",
      "    Issue Slots Busy               %         3.32\n",
      "    Issued Ipc Active     inst/cycle         0.13\n",
      "    SM Busy                        %         4.69\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 95.31%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    --------------------------- ----------- ------------\n",
      "    Metric Name                 Metric Unit Metric Value\n",
      "    --------------------------- ----------- ------------\n",
      "    Memory Throughput               Gbyte/s         2.08\n",
      "    Mem Busy                              %         0.49\n",
      "    Max Bandwidth                         %         0.83\n",
      "    L1/TEX Hit Rate                       %        11.59\n",
      "    L2 Compression Success Rate           %            0\n",
      "    L2 Compression Ratio                               0\n",
      "    L2 Hit Rate                           %        71.85\n",
      "    Mem Pipes Busy                        %         0.07\n",
      "    --------------------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Tables\n",
      "    OPT   Est. Speedup: 0.05297%                                                                                        \n",
      "          The memory access pattern for global loads from L2 might not be optimal. On average, only 28.2 of the 32      \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 91.3% of sectors missed in      \n",
      "          L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for       \n",
      "          uncoalesced global loads.                                                                                     \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Speedup: 0.07299%                                                                                        \n",
      "          The memory access pattern for global stores to DRAM might not be optimal. On average, only 28.2 of the 32     \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 0.0% of sectors missed in L2.   \n",
      "          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  \n",
      "          global stores.                                                                                                \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %         3.44\n",
      "    Issued Warp Per Scheduler                        0.03\n",
      "    No Eligible                            %        96.56\n",
      "    Active Warps Per Scheduler          warp         1.65\n",
      "    Eligible Warps Per Scheduler        warp         0.04\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.56%                                                                                    \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 29.0 cycles. This might leave hardware resources underutilized and may lead to    \n",
      "          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   \n",
      "          1.65 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        48.04\n",
      "    Warp Cycles Per Executed Instruction           cycle        60.25\n",
      "    Avg. Active Threads Per Warp                                24.58\n",
      "    Avg. Not Predicated Off Threads Per Warp                    24.13\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 55.07%                                                                                          \n",
      "          On average, each warp of this kernel spends 26.5 cycles being stalled waiting for an immediate constant cache \n",
      "          (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;        \n",
      "          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      \n",
      "          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    \n",
      "          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       \n",
      "          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   \n",
      "          threads of a warp access the same location, then constant memory can be as fast as a register access. This    \n",
      "          stall type represents about 55.1% of the total average of 48.0 cycles between issuing two instructions.       \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst         1.89\n",
      "    Executed Instructions                           inst          181\n",
      "    Avg. Issued Instructions Per Scheduler          inst         2.36\n",
      "    Issued Instructions                             inst          227\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 2.343%                                                                                          \n",
      "          This kernel executes 0 fused and 5 non-fused FP64 instructions. By converting pairs of non-fused instructions \n",
      "          to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput      \n",
      "          equivalent, the achieved FP64 performance could be increased by up to 50% (relative to its current            \n",
      "          performance). Check the Source page to identify where this kernel executes FP64 instructions.                 \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           16.38\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              24\n",
      "    Threads                                   thread             256\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.01\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 95.83%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           24\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            6\n",
      "    Theoretical Active Warps per SM        warp           48\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        13.68\n",
      "    Achieved Active Warps Per SM           warp         6.57\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 86.32%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (13.7%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle          148\n",
      "    Total DRAM Elapsed Cycles        cycle       71,680\n",
      "    Average L1 Active Cycles         cycle        71.12\n",
      "    Total L1 Elapsed Cycles          cycle       85,976\n",
      "    Average L2 Active Cycles         cycle       344.94\n",
      "    Total L2 Elapsed Cycles          cycle       55,648\n",
      "    Average SM Active Cycles         cycle        71.12\n",
      "    Total SM Elapsed Cycles          cycle       85,976\n",
      "    Average SMSP Active Cycles       cycle        68.69\n",
      "    Total SMSP Elapsed Cycles        cycle      343,904\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 5.375%                                                                                          \n",
      "          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     \n",
      "          Maximum instance value is 54.19% above the average, while the minimum instance value is 87.82% below the      \n",
      "          average.                                                                                                      \n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.07\n",
      "    Branch Instructions              inst           13\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "  matrixAddKernel1(double *, double *, double *, unsigned long long, unsigned long long) (1, 1, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         7.78\n",
      "    SM Frequency                    Ghz         1.56\n",
      "    Elapsed Cycles                cycle        3,590\n",
      "    Memory Throughput                 %         0.65\n",
      "    DRAM Throughput                   %         0.65\n",
      "    Duration                         us         2.30\n",
      "    L1/TEX Cache Throughput           %        16.63\n",
      "    L2 Cache Throughput               %         0.44\n",
      "    SM Active Cycles              cycle        72.17\n",
      "    Compute (SM) Throughput           %         0.09\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of \n",
      "          this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling   \n",
      "          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         \n",
      "          roofline analysis.                                                                                            \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Mbyte         2.10\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval          us            1\n",
      "    # Pass Groups                                    2\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     \n",
      "          samples.                                                                                                      \n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.10\n",
      "    Executed Ipc Elapsed  inst/cycle         0.00\n",
      "    Issue Slots Busy               %         3.28\n",
      "    Issued Ipc Active     inst/cycle         0.13\n",
      "    SM Busy                        %         4.62\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 95.38%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    --------------------------- ----------- ------------\n",
      "    Metric Name                 Metric Unit Metric Value\n",
      "    --------------------------- ----------- ------------\n",
      "    Memory Throughput               Gbyte/s         1.61\n",
      "    Mem Busy                              %         0.44\n",
      "    Max Bandwidth                         %         0.65\n",
      "    L1/TEX Hit Rate                       %           20\n",
      "    L2 Compression Success Rate           %            0\n",
      "    L2 Compression Ratio                               0\n",
      "    L2 Hit Rate                           %        74.60\n",
      "    Mem Pipes Busy                        %         0.07\n",
      "    --------------------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Tables\n",
      "    OPT   Est. Speedup: 0.08877%                                                                                        \n",
      "          The memory access pattern for global loads from L2 might not be optimal. On average, only 24.0 of the 32      \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 80.0% of sectors missed in      \n",
      "          L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for       \n",
      "          uncoalesced global loads.                                                                                     \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Speedup: 0.1295%                                                                                         \n",
      "          The memory access pattern for global stores to DRAM might not be optimal. On average, only 24.0 of the 32     \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 0.0% of sectors missed in L2.   \n",
      "          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  \n",
      "          global stores.                                                                                                \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %         3.27\n",
      "    Issued Warp Per Scheduler                        0.03\n",
      "    No Eligible                            %        96.73\n",
      "    Active Warps Per Scheduler          warp         1.57\n",
      "    Eligible Warps Per Scheduler        warp         0.04\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.73%                                                                                    \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 30.6 cycles. This might leave hardware resources underutilized and may lead to    \n",
      "          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   \n",
      "          1.57 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        47.93\n",
      "    Warp Cycles Per Executed Instruction           cycle        60.12\n",
      "    Avg. Active Threads Per Warp                                19.79\n",
      "    Avg. Not Predicated Off Threads Per Warp                    19.62\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 55.12%                                                                                          \n",
      "          On average, each warp of this kernel spends 26.4 cycles being stalled waiting for an immediate constant cache \n",
      "          (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;        \n",
      "          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      \n",
      "          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    \n",
      "          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       \n",
      "          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   \n",
      "          threads of a warp access the same location, then constant memory can be as fast as a register access. This    \n",
      "          stall type represents about 55.1% of the total average of 47.9 cycles between issuing two instructions.       \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Speedup: 0.03592%                                                                                        \n",
      "          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         \n",
      "          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     \n",
      "          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  \n",
      "          per cycle. This kernel achieves an average of 19.8 threads being active per cycle. This is further reduced    \n",
      "          to 19.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      \n",
      "          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  \n",
      "          execute the instructions. Try to avoid different execution paths within a warp when possible.                 \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst         1.89\n",
      "    Executed Instructions                           inst          181\n",
      "    Avg. Issued Instructions Per Scheduler          inst         2.36\n",
      "    Issued Instructions                             inst          227\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 2.309%                                                                                          \n",
      "          This kernel executes 0 fused and 5 non-fused FP64 instructions. By converting pairs of non-fused instructions \n",
      "          to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput      \n",
      "          equivalent, the achieved FP64 performance could be increased by up to 50% (relative to its current            \n",
      "          performance). Check the Source page to identify where this kernel executes FP64 instructions.                 \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           16.38\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              24\n",
      "    Threads                                   thread             256\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.01\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 95.83%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           24\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            6\n",
      "    Theoretical Active Warps per SM        warp           48\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        13.41\n",
      "    Achieved Active Warps Per SM           warp         6.44\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 86.59%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (13.4%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle          116\n",
      "    Total DRAM Elapsed Cycles        cycle       71,680\n",
      "    Average L1 Active Cycles         cycle        72.17\n",
      "    Total L1 Elapsed Cycles          cycle       86,128\n",
      "    Average L2 Active Cycles         cycle       253.75\n",
      "    Total L2 Elapsed Cycles          cycle       55,872\n",
      "    Average SM Active Cycles         cycle        72.17\n",
      "    Total SM Elapsed Cycles          cycle       86,128\n",
      "    Average SMSP Active Cycles       cycle        72.26\n",
      "    Total SMSP Elapsed Cycles        cycle      344,512\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.07\n",
      "    Branch Instructions              inst           13\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             \n",
      "\n",
      "  matrixAddKernel1(double *, double *, double *, unsigned long long, unsigned long long) (1, 1, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         7.77\n",
      "    SM Frequency                    Ghz         1.59\n",
      "    Elapsed Cycles                cycle        3,502\n",
      "    Memory Throughput                 %         0.61\n",
      "    DRAM Throughput                   %         0.61\n",
      "    Duration                         us         2.21\n",
      "    L1/TEX Cache Throughput           %        17.44\n",
      "    L2 Cache Throughput               %         0.44\n",
      "    SM Active Cycles              cycle        68.79\n",
      "    Compute (SM) Throughput           %         0.05\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
      "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of \n",
      "          this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling   \n",
      "          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         \n",
      "          roofline analysis.                                                                                            \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Mbyte         2.10\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval          us            1\n",
      "    # Pass Groups                                    2\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     \n",
      "          samples.                                                                                                      \n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.08\n",
      "    Executed Ipc Elapsed  inst/cycle         0.00\n",
      "    Issue Slots Busy               %         2.57\n",
      "    Issued Ipc Active     inst/cycle         0.10\n",
      "    SM Busy                        %         2.57\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 98.06%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    --------------------------- ----------- ------------\n",
      "    Metric Name                 Metric Unit Metric Value\n",
      "    --------------------------- ----------- ------------\n",
      "    Memory Throughput               Gbyte/s         1.51\n",
      "    Mem Busy                              %         0.44\n",
      "    Max Bandwidth                         %         0.61\n",
      "    L1/TEX Hit Rate                       %            0\n",
      "    L2 Compression Success Rate           %            0\n",
      "    L2 Compression Ratio                               0\n",
      "    L2 Hit Rate                           %        75.21\n",
      "    Mem Pipes Busy                        %         0.05\n",
      "    --------------------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Tables\n",
      "    OPT   Est. Speedup: 0.04445%                                                                                        \n",
      "          The memory access pattern for global loads from L2 might not be optimal. On average, only 28.8 of the 32      \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 100.0% of sectors missed in     \n",
      "          L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for       \n",
      "          uncoalesced global loads.                                                                                     \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    OPT   Est. Speedup: 0.06063%                                                                                        \n",
      "          The memory access pattern for global stores to DRAM might not be optimal. On average, only 28.8 of the 32     \n",
      "          bytes transmitted per sector are utilized by each thread. This applies to the 0.0% of sectors missed in L2.   \n",
      "          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  \n",
      "          global stores.                                                                                                \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %         3.40\n",
      "    Issued Warp Per Scheduler                        0.03\n",
      "    No Eligible                            %        96.60\n",
      "    Active Warps Per Scheduler          warp         1.74\n",
      "    Eligible Warps Per Scheduler        warp         0.04\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 96.6%                                                                                     \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 29.4 cycles. This might leave hardware resources underutilized and may lead to    \n",
      "          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   \n",
      "          1.74 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        51.12\n",
      "    Warp Cycles Per Executed Instruction           cycle        66.85\n",
      "    Avg. Active Threads Per Warp                                25.98\n",
      "    Avg. Not Predicated Off Threads Per Warp                    25.85\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 66.87%                                                                                          \n",
      "          On average, each warp of this kernel spends 34.2 cycles being stalled waiting for an immediate constant cache \n",
      "          (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;        \n",
      "          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      \n",
      "          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    \n",
      "          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       \n",
      "          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   \n",
      "          threads of a warp access the same location, then constant memory can be as fast as a register access. This    \n",
      "          stall type represents about 66.9% of the total average of 51.1 cycles between issuing two instructions.       \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst         1.35\n",
      "    Executed Instructions                           inst          130\n",
      "    Avg. Issued Instructions Per Scheduler          inst         1.77\n",
      "    Issued Instructions                             inst          170\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 0.9691%                                                                                         \n",
      "          This kernel executes 0 fused and 2 non-fused FP64 instructions. By converting pairs of non-fused instructions \n",
      "          to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput      \n",
      "          equivalent, the achieved FP64 performance could be increased by up to 50% (relative to its current            \n",
      "          performance). Check the Source page to identify where this kernel executes FP64 instructions.                 \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                      1\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           16.38\n",
      "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              24\n",
      "    Threads                                   thread             256\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                                0.01\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    OPT   Est. Speedup: 95.83%                                                                                          \n",
      "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              \n",
      "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
      "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
      "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
      "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
      "          description for more details on launch configurations.                                                        \n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           24\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            6\n",
      "    Theoretical Active Warps per SM        warp           48\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        10.98\n",
      "    Achieved Active Warps Per SM           warp         5.27\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 89.02%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (11.0%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle          104\n",
      "    Total DRAM Elapsed Cycles        cycle       68,608\n",
      "    Average L1 Active Cycles         cycle        68.79\n",
      "    Total L1 Elapsed Cycles          cycle       84,016\n",
      "    Average L2 Active Cycles         cycle       272.38\n",
      "    Total L2 Elapsed Cycles          cycle       54,448\n",
      "    Average SM Active Cycles         cycle        68.79\n",
      "    Total SM Elapsed Cycles          cycle       84,016\n",
      "    Average SMSP Active Cycles       cycle        52.06\n",
      "    Total SMSP Elapsed Cycles        cycle      336,064\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 5.358%                                                                                          \n",
      "          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     \n",
      "          Maximum instance value is 66.94% above the average, while the minimum instance value is 84.58% below the      \n",
      "          average.                                                                                                      \n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.08\n",
      "    Branch Instructions              inst           10\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For profiling with nvprof (Windows style) the new one  for system-level profiling\n",
    "# !nsys profile --stats=true .\\kernel1.exe inputfile.txt outputfile_cpu.txt\n",
    "# , for kernel details using Nsight Compute:\n",
    "!ncu --set full .\\kernel1.exe inputfile.txt outputfile_cpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel2: each thread produces one output matrix row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:12.967571Z",
     "iopub.status.busy": "2025-02-17T11:09:12.967270Z",
     "iopub.status.idle": "2025-02-17T11:09:12.973841Z",
     "shell.execute_reply": "2025-02-17T11:09:12.972954Z",
     "shell.execute_reply.started": "2025-02-17T11:09:12.967548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernel2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernel2.cu   \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <assert.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <fstream>\n",
    "#include <iostream>\n",
    "#include <sstream>\n",
    "#define MAX_ERR 1e-6\n",
    "__global__ void matrixAddKernel1(double* C, double* A, double* B, size_t rows, size_t cols) {\n",
    "    size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (row < rows) {\n",
    "        for(size_t col =0 ; col < cols; col++){\n",
    "              size_t idx = row * cols + col;\n",
    "            C[idx] = A[idx] + B[idx];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cudaError_t addMatricesWithCuda(double* C, double* A, double* B, size_t rows, size_t cols) {\n",
    "    double* dev_A = nullptr;\n",
    "    double* dev_B = nullptr;\n",
    "    double* dev_C = nullptr;\n",
    "    cudaError_t cudaStatus;\n",
    "\n",
    "    // Allocate GPU buffers\n",
    "    size_t size = rows * cols * sizeof(double);  // Changed from size_t to double\n",
    "    \n",
    "    cudaStatus = cudaMalloc((void**)&dev_C, size);\n",
    "    \n",
    "    cudaStatus = cudaMalloc((void**)&dev_A, size);\n",
    "\n",
    "    cudaStatus = cudaMalloc((void**)&dev_B, size);\n",
    "\n",
    "    // Copy input matrices from host memory to GPU buffers\n",
    "    cudaStatus = cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);    \n",
    "    cudaStatus = cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    dim3 threadsPerBlock(16, 16); // i just did what Ta and cuda said the best to use\n",
    "    dim3 numBlocks((cols + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                   (rows + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrixAddKernel1<<<numBlocks, threadsPerBlock>>>(dev_C, dev_A, dev_B, rows, cols);\n",
    "\n",
    "    // Copy output matrix from GPU buffer to host memory\n",
    "    cudaStatus = cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    return cudaStatus;\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "\n",
    "    FILE *file_reading;\n",
    "    int numberOfTests;\n",
    "    size_t  rows, cols;\n",
    "    // Open the file in read mode\n",
    "    file_reading = fopen(argv[1], \"r\");\n",
    "    if (file_reading == NULL) {\n",
    "        printf(\"Error opening file!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    // Read number of tests\n",
    "    fscanf(file_reading, \"%d\",&numberOfTests);\n",
    "for(size_t i=0;i<numberOfTests;i++){\n",
    "    \n",
    "    // Read matrix dimensions\n",
    "    fscanf(file_reading, \"%zu %zu\", &rows, &cols);\n",
    "\n",
    "    // Allocate host matrices\n",
    "    double* A = (double*)malloc(sizeof(double) * rows * cols);  // Changed from size_t to double\n",
    "    double* B = (double*)malloc(sizeof(double) * rows * cols);\n",
    "    double* C = (double*)malloc(sizeof(double) * rows * cols);\n",
    "\n",
    "    // Read matrices A and B\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &A[i]);\n",
    "    }\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &B[i]);\n",
    "    }\n",
    "    \n",
    " \n",
    "\n",
    "    // Add matrices using CUDA\n",
    "    cudaError_t cudaStatus = addMatricesWithCuda(C, A, B, rows, cols);\n",
    "\n",
    "    // Verification\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        assert(fabs(C[i] - A[i] - B[i]) < MAX_ERR);\n",
    "    }\n",
    "\n",
    "    printf(\"Vector addition completed successfully!\\n\");\n",
    "\n",
    "    // Write results to output file\n",
    "    FILE *file_writing;\n",
    "    file_writing= fopen(argv[2], \"w\"); // Open file for writing\n",
    "    if (file_writing == NULL) {\n",
    "        perror(\"Error opening file\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "\n",
    "    // Write matrix C\n",
    "    for (size_t i = 0; i < rows; i++) {\n",
    "        for (size_t j = 0; j < cols; j++) {\n",
    "              printf(\"%.3f \", C[i * cols + j]);    \n",
    "            fprintf(file_writing, \"%.3f \", C[i * cols + j]); // Write double with 2 decimal places\n",
    "        }\n",
    "         printf(\"\\n\");  \n",
    "        fprintf(file_writing, \"\\n\"); // New line after each row\n",
    "    }\n",
    "    fclose(file_writing);\n",
    "\n",
    "\n",
    "    // Cleanup\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "}\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:12.974990Z",
     "iopub.status.busy": "2025-02-17T11:09:12.974691Z",
     "iopub.status.idle": "2025-02-17T11:09:15.514352Z",
     "shell.execute_reply": "2025-02-17T11:09:15.513478Z",
     "shell.execute_reply.started": "2025-02-17T11:09:12.974968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel2.cu\n",
      "tmpxft_00003b94_00000000-10_kernel2.cudafe1.cpp\n",
      "   Creating library kernel2.lib and object kernel2.exp\n"
     ]
    }
   ],
   "source": [
    "!nvcc kernel2.cu -o kernel2\n",
    "!nvprof ./kernel2 inputfile.txt outputfile_kernel2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel3: each thread produces one output matrix col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:15.516579Z",
     "iopub.status.busy": "2025-02-17T11:09:15.516312Z",
     "iopub.status.idle": "2025-02-17T11:09:15.522944Z",
     "shell.execute_reply": "2025-02-17T11:09:15.521897Z",
     "shell.execute_reply.started": "2025-02-17T11:09:15.516556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernel3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernel3.cu   \n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <assert.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "#include <fstream>\n",
    "#include <iostream>\n",
    "#include <sstream>\n",
    "#define MAX_ERR 1e-6\n",
    "__global__ void matrixAddKernel1(double* C, double* A, double* B, size_t rows, size_t cols) {\n",
    "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if ( col < cols) {\n",
    "        for(size_t row = 0; row< rows ; row++){\n",
    "            size_t idx = row * cols + col;\n",
    "            C[idx] = A[idx] + B[idx];\n",
    "        }\n",
    "   \n",
    "    }\n",
    "}\n",
    "\n",
    "cudaError_t addMatricesWithCuda(double* C, double* A, double* B, size_t rows, size_t cols) {\n",
    "    double* dev_A = nullptr;\n",
    "    double* dev_B = nullptr;\n",
    "    double* dev_C = nullptr;\n",
    "    cudaError_t cudaStatus;\n",
    "\n",
    "    // Allocate GPU buffers\n",
    "    size_t size = rows * cols * sizeof(double);  // Changed from size_t to double\n",
    "    \n",
    "    cudaStatus = cudaMalloc((void**)&dev_C, size);\n",
    "    \n",
    "    cudaStatus = cudaMalloc((void**)&dev_A, size);\n",
    "\n",
    "    cudaStatus = cudaMalloc((void**)&dev_B, size);\n",
    "\n",
    "    // Copy input matrices from host memory to GPU buffers\n",
    "    cudaStatus = cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);    \n",
    "    cudaStatus = cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    dim3 threadsPerBlock(16, 16); // i just did what Ta and cuda said the best to use\n",
    "    dim3 numBlocks((cols + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                   (rows + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrixAddKernel1<<<numBlocks, threadsPerBlock>>>(dev_C, dev_A, dev_B, rows, cols);\n",
    "\n",
    "    // Copy output matrix from GPU buffer to host memory\n",
    "    cudaStatus = cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    return cudaStatus;\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "\n",
    "    FILE *file_reading;\n",
    "    int numberOfTests;\n",
    "    size_t  rows, cols;\n",
    "    // Open the file in read mode\n",
    "    file_reading = fopen(argv[1], \"r\");\n",
    "    if (file_reading == NULL) {\n",
    "        printf(\"Error opening file!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    // Read number of tests\n",
    "    fscanf(file_reading, \"%d\",&numberOfTests);\n",
    "for(size_t i=0;i<numberOfTests;i++){\n",
    "    \n",
    "    // Read matrix dimensions\n",
    "    fscanf(file_reading, \"%zu %zu\", &rows, &cols);\n",
    "\n",
    "    // Allocate host matrices\n",
    "    double* A = (double*)malloc(sizeof(double) * rows * cols);  // Changed from size_t to double\n",
    "    double* B = (double*)malloc(sizeof(double) * rows * cols);\n",
    "    double* C = (double*)malloc(sizeof(double) * rows * cols);\n",
    "\n",
    "    // Read matrices A and B\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &A[i]);\n",
    "    }\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        fscanf(file_reading, \"%lf\", &B[i]);\n",
    "    }\n",
    "    \n",
    " \n",
    "\n",
    "    // Add matrices using CUDA\n",
    "    cudaError_t cudaStatus = addMatricesWithCuda(C, A, B, rows, cols);\n",
    "\n",
    "    // Verification\n",
    "    for (size_t i = 0; i < rows * cols; i++) {\n",
    "        assert(fabs(C[i] - A[i] - B[i]) < MAX_ERR);\n",
    "    }\n",
    "\n",
    "    printf(\"Vector addition completed successfully!\\n\");\n",
    "\n",
    "    // Write results to output file\n",
    "    FILE *file_writing;\n",
    "    file_writing= fopen(argv[2], \"w\"); // Open file for writing\n",
    "    if (file_writing == NULL) {\n",
    "        perror(\"Error opening file\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "\n",
    "    // Write matrix C\n",
    "    for (size_t i = 0; i < rows; i++) {\n",
    "        for (size_t j = 0; j < cols; j++) {\n",
    "            printf(\"%.3f \", C[i * cols + j]);    \n",
    "            fprintf(file_writing, \"%.3f \", C[i * cols + j]); // Write double with 2 decimal places\n",
    "        }\n",
    "        printf(\"\\n\");  \n",
    "        fprintf(file_writing, \"\\n\"); // New line after each row\n",
    "    }\n",
    "    fclose(file_writing);\n",
    "\n",
    "\n",
    "    // Cleanup\n",
    "    free(A);\n",
    "    free(B);\n",
    "    free(C);\n",
    "}\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T11:09:15.524342Z",
     "iopub.status.busy": "2025-02-17T11:09:15.524009Z",
     "iopub.status.idle": "2025-02-17T11:09:18.084042Z",
     "shell.execute_reply": "2025-02-17T11:09:18.083151Z",
     "shell.execute_reply.started": "2025-02-17T11:09:15.524310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel3.cu\n",
      "tmpxft_00001f04_00000000-10_kernel3.cudafe1.cpp\n",
      "   Creating library kernel3.lib and object kernel3.exp\n"
     ]
    }
   ],
   "source": [
    "!nvcc kernel3.cu -o kernel3\n",
    "!nvprof ./kernel3 inputfile.txt outputfile_kernel3.txt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

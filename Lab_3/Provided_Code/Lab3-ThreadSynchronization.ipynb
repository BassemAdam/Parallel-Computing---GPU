{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeYRECDhwjCS"
      },
      "source": [
        "###**Intrinsic Functions**\n",
        "- Specialized functions provided by the CUDA programming model. They are callable only from the device. They do not need to include any additional headers in your program.\n",
        "- These functions often offer an alternative to standard functions that are faster but may have less numerical accuracy, they are majorly used in mathematical functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F64T3RiV76Mg"
      },
      "source": [
        "\n",
        "### **Thread Synchronization**\n",
        "\n",
        "Threads **within a block** can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() **intrinsic** function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing reverse1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile reverse1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void staticReverse(int *d, int n)\n",
        "{\n",
        "  __shared__ int s[64];\n",
        "  int t = threadIdx.x;\n",
        "  int tr = n-t-1;\n",
        "  s[t] = d[t];\n",
        "  __syncthreads();\n",
        "  d[t] = s[tr];\n",
        "}\n",
        "\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  const int n = 64;\n",
        "  int a[n], r[n], d[n];\n",
        "\n",
        "  for (int i = 0; i < n; i++) {\n",
        "    a[i] = i;\n",
        "    r[i] = n-i-1;\n",
        "    d[i] = 0;\n",
        "  }\n",
        "\n",
        "  int *d_d;\n",
        "  cudaMalloc(&d_d, n * sizeof(int)); \n",
        "\n",
        "  // run version with static shared memory\n",
        "  cudaMemcpy(d_d, a, n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "  staticReverse<<<1,n>>>(d_d, n);\n",
        "  cudaMemcpy(d, d_d, n*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "  for (int i = 0; i < n; i++) \n",
        "    if (d[i] != r[i]) printf(\"Error: d[%d]!=r[%d] (%d, %d)n\", i, i, d[i], r[i]);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile reverse2.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void dynamicReverse(int *d, int n)\n",
        "{\n",
        "  extern __shared__ int s[];\n",
        "  int t = threadIdx.x;\n",
        "  int tr = n-t-1;\n",
        "  s[t] = d[t];\n",
        "  __syncthreads();\n",
        "  d[t] = s[tr];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  const int n = 64;\n",
        "  int a[n], r[n], d[n];\n",
        "\n",
        "  for (int i = 0; i < n; i++) {\n",
        "    a[i] = i;\n",
        "    r[i] = n-i-1;\n",
        "    d[i] = 0;\n",
        "  }\n",
        "\n",
        "  int *d_d;\n",
        "  cudaMalloc(&d_d, n * sizeof(int)); \n",
        "\n",
        "  // run dynamic shared memory version\n",
        "  cudaMemcpy(d_d, a, n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "  dynamicReverse<<<1,n,n*sizeof(int)>>>(d_d, n);\n",
        "  cudaMemcpy(d, d_d, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "  for (int i = 0; i < n; i++) \n",
        "    if (d[i] != r[i]) printf(\"Error: d[%d]!=r[%d] (%d, %d)n\", i, i, d[i], r[i]);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Dynamic Shared Memory**\n",
        "\n",
        "Can be used when the amount of shared memory is not known at compile time. In this case the shared memory allocation size per thread block must be specified (in bytes) using an optional third execution configuration parameter, as follows:\n",
        "\n",
        "*dynamicReverse<<<1, n, n* * *sizeof(int)>>>(d_d, n)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What if you need multiple dynamically sized arrays in a single kernel? You must declare a single extern unsized array as before, and use pointers into it to divide it into multiple arrays, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extern __shared__ int s[];\n",
        "int *integerData = s;                        // nI ints\n",
        "float *floatData = (float*)&integerData[nI]; // nF floats\n",
        "char *charData = (char*)&floatData[nF];      // nC chars\n",
        "\n",
        "// In the kernel launch, specify the total shared memory needed:\n",
        "myKernel<<<gridSize, blockSize, nI*sizeof(int)+nF*sizeof(float)+nC*sizeof(char)>>>(...);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drdqiLhiP56h"
      },
      "source": [
        "The __syncthreads() statement in the for-loop ensures that all partial sums for the previous iteration have been generated and before any one of the threads is allowed to begin the current iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-IPfGzZCiEA"
      },
      "source": [
        "#### **Thread Divergence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu1SU-ZDAnNr"
      },
      "outputs": [],
      "source": [
        "# Consider this example\n",
        "\n",
        "# Does this code work properly? why?\n",
        "if{\n",
        "     ...\n",
        "     __syncthreads();\n",
        "}else{\n",
        "     ...\n",
        "     __syncthreads();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCsWXdb5Avci"
      },
      "source": [
        "If a thread in a block executes the then-path and another executes the else-path, they would be waiting at different barrier synchronization points and end up waiting for each other forever. so if __syncthreads() exists in the kernel, it must be executed by all threads. In this sense, the above code can be fixed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLY4TIFoCRoL"
      },
      "outputs": [],
      "source": [
        "if{\n",
        "     ...\n",
        "}\n",
        "else{\n",
        "     ...\n",
        "}\n",
        "__syncthreads();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fla-gd-6K-mn"
      },
      "source": [
        "### **Let's Practise Together**\n",
        "\n",
        "modify the last requirement so that the last 1d vector summation is done using the following ways and show their profiling:\n",
        "- Use only 1 block for your kernel and let the CPU handle the final sum.\n",
        "- Use only 1 block for your kernal and let one thread to handle the final sum.\n",
        "- Use multiple blocks for your kernal and let the CPU handle the final sum.\n",
        "\n",
        "IF YOU HAVE ANY INQUIRIES PLEASE DO NOT HESITATE TO CONTACT ME\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
